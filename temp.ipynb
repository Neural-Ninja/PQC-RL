{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd21ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    dev = qml.device(\"lightning.gpu\", wires=1)\n",
    "except:\n",
    "    dev = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "class QuantumPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Parameter(torch.tensor(0.01))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.clamp(obs[:, 0], -1.0, 1.0) * np.pi\n",
    "\n",
    "        @qml.qnode(dev, interface=\"torch\")\n",
    "        def circuit(x_, theta_):\n",
    "            qml.RX(x_, wires=0)\n",
    "            qml.RY(theta_, wires=0)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "        output = torch.stack([circuit(x[i], self.theta) for i in range(len(x))])\n",
    "        probs = torch.sigmoid(output)\n",
    "        return probs\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "policy = QuantumPolicy()\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "episodes = 100\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = policy(obs_tensor)\n",
    "\n",
    "        m = torch.distributions.Bernoulli(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(int(action.item()))\n",
    "        done = terminated or truncated\n",
    "\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "    loss = -torch.sum(torch.stack(log_probs) * returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode+1:3d}: Total Reward = {sum(rewards):.0f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e401fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "try:\n",
    "    dev = qml.device(\"lightning.gpu\", wires=1)\n",
    "except:\n",
    "    dev = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "class QuantumPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Parameter(torch.tensor(0.01))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.clamp(obs[:, 0], -1.0, 1.0) * np.pi\n",
    "\n",
    "        @qml.qnode(dev, interface=\"torch\")\n",
    "        def circuit(x_, theta_):\n",
    "            qml.RX(x_, wires=0)\n",
    "            qml.RY(theta_, wires=0)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "        output = torch.stack([circuit(x[i], self.theta) for i in range(len(x))])\n",
    "        probs = torch.sigmoid(output)\n",
    "        return probs\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "policy = QuantumPolicy()\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "episodes = 100\n",
    "gamma = 0.99\n",
    "\n",
    "frames = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = policy(obs_tensor)\n",
    "\n",
    "        m = torch.distributions.Bernoulli(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(int(action.item()))\n",
    "        done = terminated or truncated\n",
    "\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        time.sleep(0.02)\n",
    "\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "    loss = -torch.sum(torch.stack(log_probs) * returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode+1:3d}: Total Reward = {sum(rewards):.0f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "gif_filename = \"cartpole_training1.gif\"\n",
    "imageio.mimsave(gif_filename, frames, duration=0.15)\n",
    "print(f\"GIF saved as {gif_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac9f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a8a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW-PQC Output (Expectation of Pauli-Z operators): tensor([ 0.4432,  0.1305, -0.0617,  0.8447], grad_fn=<StackBackward0>)\n",
      "SOFTMAX-PQC Output (Softmax of weighted Hermitian operators): tensor([0.3485, 0.2796, 0.2915, 0.0804], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PQC(nn.Module):\n",
    "    def __init__(self, n_qubits, n_depth):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_depth = n_depth\n",
    "        self.s = nn.Parameter(torch.randn(n_qubits))\n",
    "        self.theta = nn.Parameter(torch.randn(n_qubits))\n",
    "        self.w = nn.Parameter(torch.randn(n_qubits))\n",
    "        self.beta = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, s):\n",
    "        @qml.qnode(dev, interface=\"torch\")\n",
    "        def circuit(s, theta, w):\n",
    "            for depth in range(self.n_depth):\n",
    "                for i in range(self.n_qubits):\n",
    "                    qml.Hadamard(wires=i)\n",
    "                    qml.RZ(s[i], wires=i)\n",
    "                    qml.RY(s[i], wires=i)\n",
    "                for i in range(self.n_qubits):\n",
    "                    qml.RX(theta[i], wires=i)\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "        raw_expectations = circuit(s, self.theta, self.w)\n",
    "        \n",
    "        raw_expectations_tensor = torch.stack(raw_expectations)\n",
    "        weighted_expectations = raw_expectations_tensor * self.w\n",
    "        \n",
    "        softmax_output = torch.softmax(weighted_expectations, dim=0)\n",
    "\n",
    "        return raw_expectations_tensor, softmax_output\n",
    "\n",
    "n_qubits = 4\n",
    "n_depth = 3\n",
    "dev = qml.device(\"lightning.gpu\", wires=n_qubits)\n",
    "\n",
    "pqc = PQC(n_qubits=n_qubits, n_depth=n_depth)\n",
    "\n",
    "s = torch.tensor([0.5, -0.2, 0.1, -0.3], dtype=torch.float32)\n",
    "\n",
    "raw_pqc_output, softmax_pqc_output = pqc(s)\n",
    "\n",
    "print(\"RAW-PQC Output (Expectation of Pauli-Z operators):\", raw_pqc_output)\n",
    "print(\"SOFTMAX-PQC Output (Softmax of weighted Hermitian operators):\", softmax_pqc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f7e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212ad8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15c0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f205db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pennylane as qml\n",
    "from typing import List, Tuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits: int, n_layers: int):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self.qnode = qml.QNode(self.circuit, self.dev, interface=\"torch\")\n",
    "        self.params = nn.Parameter(\n",
    "            torch.randn(n_layers, n_qubits, 3, requires_grad=True, device=device)\n",
    "        )\n",
    "    \n",
    "    def circuit(self, inputs, params):\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RX(inputs[i], wires=i)\n",
    "        for layer in range(self.n_layers):\n",
    "            for qubit in range(self.n_qubits):\n",
    "                qml.RX(params[layer, qubit, 0], wires=qubit)\n",
    "                qml.RY(params[layer, qubit, 1], wires=qubit)\n",
    "                qml.RZ(params[layer, qubit, 2], wires=qubit)\n",
    "            for qubit in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[qubit, qubit + 1])\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "class QuantumPolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_qubits: int, n_layers: int, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.pre_process = nn.Linear(input_dim, n_qubits)\n",
    "        self.quantum_circuit = QuantumCircuit(n_qubits, n_layers)\n",
    "        self.post_process = nn.Linear(n_qubits, output_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "        x = torch.tanh(self.pre_process(x))\n",
    "        q_out = self.quantum_circuit.qnode(x.detach().cpu().numpy(), \n",
    "                                           self.quantum_circuit.params.detach().cpu().numpy())\n",
    "        q_out = torch.tensor(q_out, dtype=torch.float32, device=device)\n",
    "        action_probs = F.softmax(self.post_process(q_out), dim=-1)\n",
    "        return action_probs\n",
    "    \n",
    "class QuantumAdvantagePolicy:\n",
    "    def __init__(self, env_name: str, n_qubits: int = 4, n_layers: int = 2):\n",
    "        self.env = gym.make(env_name)\n",
    "        if isinstance(self.env.observation_space, gym.spaces.Box):\n",
    "            self.input_dim = np.prod(self.env.observation_space.shape)\n",
    "        elif isinstance(self.env.observation_space, gym.spaces.Discrete):\n",
    "            self.input_dim = self.env.observation_space.n\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported observation space type: {type(self.env.observation_space)}\")\n",
    "        self.output_dim = self.env.action_space.n\n",
    "        self.policy = QuantumPolicyNetwork(\n",
    "            n_qubits=n_qubits,\n",
    "            n_layers=n_layers,\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim\n",
    "        ).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=0.01)\n",
    "\n",
    "    def select_action(self, state) -> Tuple[int, torch.Tensor]:\n",
    "        if isinstance(state, tuple):\n",
    "            state = state\n",
    "        if isinstance(state, list):\n",
    "            state = np.array(state)\n",
    "        state = np.asarray(state).reshape(-1)\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        probs = self.policy(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item(), action_dist.log_prob(action)\n",
    "    \n",
    "    def train_episode(self) -> float:\n",
    "        state, _ = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_log_probs = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):\n",
    "            action, log_prob = self.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "            episode_log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        returns = self._calculate_returns(rewards, gamma=0.99)\n",
    "        policy_loss = self._update_policy(episode_log_probs, returns)\n",
    "        return episode_reward\n",
    "    \n",
    "    def _calculate_returns(self, rewards: List[float], gamma: float) -> torch.Tensor:\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, device=device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        return returns\n",
    "    \n",
    "    def _update_policy(self, log_probs: List[torch.Tensor], returns: torch.Tensor) -> float:\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return policy_loss.item()\n",
    "\n",
    "def train_quantum_policy(env_name: str, episodes: int = 1000):\n",
    "    quantum_policy = QuantumAdvantagePolicy(env_name)\n",
    "    for episode in range(episodes):\n",
    "        episode_reward = quantum_policy.train_episode()\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"CartPole-v1\"\n",
    "    train_quantum_policy(env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1ff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d4e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66edf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
